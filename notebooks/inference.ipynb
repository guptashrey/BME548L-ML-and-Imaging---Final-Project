{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2183ab69-16a0-4cd5-b6ab-94f82df966c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.modeling import build_model,build_resnet_backbone,build_backbone\n",
    "from detectron2.structures import ImageList, Instances\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import timm\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7069701-29ba-43d6-b3a8-55982826e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Encoder(VisionTransformer):\n",
    "    def __init__(self, pretrained = False, pretrained_model = None, img_size=224, patch_size=16, in_chans=3, num_classes=1, embed_dim=768, depth=12,\n",
    "                  num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                  drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super(Transformer_Encoder, self).__init__(img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=1000, embed_dim=embed_dim, depth=depth,\n",
    "                  num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n",
    "                  drop_path_rate=drop_path_rate, hybrid_backbone=hybrid_backbone, norm_layer=norm_layer)\n",
    "        \n",
    "        self.num_classes = 1\n",
    "        self.dispatcher = {\n",
    "            'vit_small_patch16_224': vit_small_patch16_224,\n",
    "            'vit_base_patch16_224': vit_base_patch16_224,\n",
    "            'vit_large_patch16_224': vit_large_patch16_224,\n",
    "            'vit_base_patch16_384': vit_base_patch16_384,\n",
    "            'vit_base_patch32_384': vit_base_patch32_384,\n",
    "            'vit_large_patch16_384': vit_large_patch16_384,\n",
    "            'vit_large_patch32_384': vit_large_patch32_384,\n",
    "            'vit_large_patch16_224' : vit_large_patch16_224,\n",
    "            'vit_large_patch32_384': vit_large_patch32_384,\n",
    "            'vit_small_resnet26d_224': vit_small_resnet26d_224,\n",
    "            'vit_small_resnet50d_s3_224': vit_small_resnet50d_s3_224,\n",
    "            'vit_base_resnet26d_224' : vit_base_resnet26d_224,\n",
    "            'vit_base_resnet50d_224' : vit_base_resnet50d_224,\n",
    "        }\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.pretrained = pretrained\n",
    "        if pretrained:\n",
    "            self.load_weights()\n",
    "        self.head = nn.Identity()\n",
    "        self.encoder_out = [1,2,3,4,5]\n",
    "\n",
    "    def forward_features(self, x):\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        for i,blk in enumerate(self.blocks,1):\n",
    "            x = blk(x)\n",
    "            if i in self.encoder_out:\n",
    "                features.append(x)\n",
    "\n",
    "        for i in range(len(features)):\n",
    "            features[i] = self.norm(features[i])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = self.forward_features(x)\n",
    "        return features\n",
    "    \n",
    "    def load_weights(self):\n",
    "        model = None\n",
    "        try:\n",
    "            model = self.dispatcher[self.pretrained_model](pretrained=True)\n",
    "        except:\n",
    "            print('could not not load model')\n",
    "        if model == None:\n",
    "            return\n",
    "        # try:\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        print(\"successfully loaded weights!!!\")\n",
    "        \n",
    "        # except:\n",
    "        #     print(\"Could not load weights. Parameters should match!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69f85ad-a56c-4d9d-8b31-e57b5f22844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\"))\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.GAMMA = 0.05\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1   \n",
    "\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output/model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.001\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527d56b1-49c8-466b-bf85-59cbc21cfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = \"../TCIA_SegPC_dataset/test/x/\"\n",
    "pred_root = \"./coco_test/\"\n",
    "names = os.listdir(img_root)\n",
    "thresh = 0\n",
    "var= 1\n",
    "\n",
    "res_size=(1080,1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e958d1-acc4-40a2-875a-9fa4ee56b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 29/277 [00:55<07:56,  1.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m         tmp_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mtmp_mask\n\u001b[1;32m     20\u001b[0m         tmp_mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(tmp_mask, orig_shape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_NEAREST)\n\u001b[0;32m---> 21\u001b[0m         \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_root\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.bmp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name in tqdm(names):\n",
    "\n",
    "    var+=1\n",
    "\n",
    "    index = name[:-4]\n",
    "    \n",
    "    im = cv2.imread(img_root+name)\n",
    "    orig_shape = im.shape[0:2]\n",
    "    im = cv2.resize(im, res_size[::-1],interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    outputs = predictor(im)\n",
    "    scores = outputs['instances'].to('cpu').scores.numpy()\n",
    "    pred_masks = outputs['instances'].to('cpu').pred_masks.numpy()\n",
    "    count = 1\n",
    "    for i in range(len(scores)):\n",
    "        \n",
    "        if scores[i]>=thresh:\n",
    "            tmp_mask = pred_masks[i].astype('uint8')\n",
    "            tmp_mask = 255*tmp_mask\n",
    "            tmp_mask = cv2.resize(tmp_mask, orig_shape[::-1],interpolation=cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(pred_root+index+'_'+str(count)+'.bmp', tmp_mask)\n",
    "            count+=1\n",
    "    if count==1:\n",
    "        tmp_mask = np.zeros(res_size)\n",
    "        cv2.imwrite(pred_root+index+'_1.bmp', tmp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a11bb615-f718-49da-8dc4-a774c6c7e6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./coco_test/2419_101.bmp'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_root+index+'_'+str(count)+'.bmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f40ac66-3f7f-4820-b45f-ab49231db112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86861195-5887-4bf9-9850-a3a013ecd834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bme2",
   "language": "python",
   "name": "bme2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
