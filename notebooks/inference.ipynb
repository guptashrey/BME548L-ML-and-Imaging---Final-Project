{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f9b0f-a152-483b-84a0-4cf2ec864a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library imports\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import detectron2\n",
    "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data.transforms import Transform\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model,build_resnet_backbone,build_backbone\n",
    "from detectron2.structures import ImageList, Instances\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from skimage import io\n",
    "from shapely.geometry import box\n",
    "\n",
    "## Local Imports\n",
    "from transformations import *\n",
    "from helpers import *\n",
    "\n",
    "## Setup logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "## Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf227b-94e8-4c84-b23f-632cd6c40f6d",
   "metadata": {},
   "source": [
    "## Transformation Functions\n",
    "\n",
    "- Illumination: We will try to simluate the absorbtion of light wave (amplitude + phase shift) by simulating 12 microcope LED's at different angle of illumination. This would be a custom layer with optimizable weights\n",
    "- Contrast normalization: The myeloma cells have varying levels of contrast as compared to other cells and tissues, so normalizing the contrast can help them be more visible\n",
    "- Morphological operations: Erosion or dilation operations can smooth the edges of the cells can help us detect the cancer cells better\n",
    "- Gradient Filters: Sobel filter can help identify the boundaries of the cells better\n",
    "- Color Channels: Manipulate the different color channels (RGB) of the image by enhancing the effect of either red, green, or blue channel\n",
    "- Blur Filter: Try the Gaussian blur filters and Median blur filter to smooth the image and reduce the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6124e5-0ec9-4fd2-a84d-f31f31787856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter Options:\n",
    "\n",
    "# 1). IlluminationSimulation\n",
    "# 2). ContrastNormalization\n",
    "# 3). Dilation, Erosion\n",
    "# 4). SobelFilter\n",
    "# 5). EnhanceRedColor, EnhanceGreenColor, EnhanceBlueColor,\n",
    "# 6). MedianFilter, GaussianBlur\n",
    "\n",
    "TRANSFORM_TYPE = 'EnhanceBlueColor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1c562-f259-465a-bdf1-55c39ab8824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup configuration\n",
    "\n",
    "#\n",
    "os.environ['transform_type'] = TRANSFORM_TYPE+\".zip\"\n",
    "final_weights = f\"../../../sg623/BME548L-ML-and-Imaging-Final-Project/outputs/{TRANSFORM_TYPE}/model_final.pth\"\n",
    "\n",
    "\n",
    "img_root = \"../TCIA_SegPC_dataset/test/x/\"\n",
    "pred_root = f\"./{TRANSFORM_TYPE}_preds/\"\n",
    "final_pred_root = f\"./{TRANSFORM_TYPE}_final_preds/\"\n",
    "os.environ['model_final_results'] = final_pred_root\n",
    "\n",
    "# Delete predictions directory if exists\n",
    "if os.path.exists(pred_root):\n",
    "    shutil.rmtree(pred_root)\n",
    "os.makedirs(pred_root, exist_ok=False)\n",
    "if os.path.exists(final_pred_root):\n",
    "    shutil.rmtree(final_pred_root)\n",
    "os.makedirs(final_pred_root, exist_ok=False)\n",
    "\n",
    "\n",
    "names = os.listdir(img_root)\n",
    "thresh = 0.5\n",
    "res_size=(1080,1440)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d2e91-fa8d-414a-8cca-47059c2eb3f0",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eba6b6-2d12-4d3b-a733-7c00bdf8a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORM_TYPE == \"IlluminationSimulation\":\n",
    "    class NonNegativeConv2d(nn.Conv2d):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "            super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            self.weight.data.clamp_min_(0.0)\n",
    "            return nn.functional.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "    # Define netowrk architecture\n",
    "    @BACKBONE_REGISTRY.register()\n",
    "    class Effb5(Backbone):\n",
    "        def __init__(self, cfg, input_shape):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Set up the illumination layer\n",
    "            self.illumination = NonNegativeConv2d(in_channels=12, out_channels=12, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            torch.nn.init.normal_(self.illumination.weight, mean=0.0, std=0.05)\n",
    "            \n",
    "            in_channels = 1\n",
    "            encoder_name = 'timm-efficientnet-b5'\n",
    "            encoder_depth = 5\n",
    "            encoder_weights = 'noisy-student'\n",
    "            self.encoder = get_encoder(encoder_name,\n",
    "                    in_channels=in_channels,\n",
    "                    depth=encoder_depth,\n",
    "                    weights=encoder_weights)\n",
    "            self.channels = self.encoder.out_channels\n",
    "            self.conv = nn.ModuleList(\n",
    "                [nn.Conv2d(self.channels[i],256,3,stride = 2, padding = 1) for i in range(len(self.channels))]\n",
    "            )\n",
    "            self.names = [\"p\"+str(i+1) for i in range(6)]\n",
    "            \n",
    "        def forward(self, image):\n",
    "\n",
    "            illuminated_image = torch.sum(self.illumination(image), dim=1, keepdim=True)\n",
    "            features = self.encoder(illuminated_image)\n",
    "            out = {self.names[i]: self.conv[i](features[i]) for i in range(1, len(features))}\n",
    "            return out\n",
    "\n",
    "        def output_shape(self):\n",
    "            out_shape = {self.names[i]: ShapeSpec(channels =256, stride = 2**(i+1)) for i in range(1, len(self.names))}\n",
    "            return out_shape\n",
    "        \n",
    "else:\n",
    "    # Define netowrk architecture\n",
    "    @BACKBONE_REGISTRY.register()\n",
    "    class Effb5(Backbone):\n",
    "        def __init__(self, cfg, input_shape):\n",
    "            super().__init__()\n",
    "            \n",
    "            in_channels = 3\n",
    "            encoder_name = 'timm-efficientnet-b5'\n",
    "            encoder_depth = 5\n",
    "            encoder_weights = 'noisy-student'\n",
    "            self.encoder = get_encoder(encoder_name,\n",
    "                    in_channels=in_channels,\n",
    "                    depth=encoder_depth,\n",
    "                    weights=encoder_weights)\n",
    "            self.channels = self.encoder.out_channels\n",
    "            self.conv = nn.ModuleList(\n",
    "                [nn.Conv2d(self.channels[i],256,3,stride = 2, padding = 1) for i in range(len(self.channels))]\n",
    "            )\n",
    "            self.names = [\"p\"+str(i+1) for i in range(6)]\n",
    "            \n",
    "        def forward(self, image):\n",
    "\n",
    "            features = self.encoder(image)\n",
    "            out = {self.names[i]: self.conv[i](features[i]) for i in range(1, len(features))}\n",
    "            return out\n",
    "\n",
    "        def output_shape(self):\n",
    "            out_shape = {self.names[i]: ShapeSpec(channels =256, stride = 2**(i+1)) for i in range(1, len(self.names))}\n",
    "            return out_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4900d6-3af9-4956-91dd-1bc324f6e2e2",
   "metadata": {},
   "source": [
    "## Predictor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24980e-15a7-4c26-913a-5639b8c8d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultPredictor:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, transformation):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Model conifguration object\n",
    "        self.cfg = cfg.clone()\n",
    "        self.model = build_model(self.cfg)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "    \n",
    "        # Load model from checkpoint\n",
    "        checkpointer = DetectionCheckpointer(self.model)\n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "        \n",
    "        # Initialize transformation\n",
    "        self.aug = transformation[0]\n",
    "        \n",
    "        # Get inpur format\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            \n",
    "            height, width = original_image.shape[:2]\n",
    "            if not isinstance(transformation[0], type(NoOpTransform())):\n",
    "                image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "            else:\n",
    "                image = original_image\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            \n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf2ca7-21bc-483f-8dc6-b86ae3b7c9de",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133ca18-42c7-4b4d-860d-30cb864f6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
    "cfg.MODEL.BACKBONE.NAME = \"Effb5\"\n",
    "# Load model weights\n",
    "cfg.MODEL.WEIGHTS = final_weights\n",
    "\n",
    "if TRANSFORM_TYPE == \"IlluminationSimulation\":\n",
    "    # Additional parameters for \"IlluminationSimulation Transform\"\n",
    "    cfg.MODEL.PIXEL_MEAN = [0.5]\n",
    "    cfg.MODEL.PIXEL_STD = [1.0]\n",
    "\n",
    "# Inisitalize the transformation    \n",
    "transformation = select_transformation(TRANSFORM_TYPE)\n",
    "\n",
    "# Create predictor object\n",
    "predictor = DefaultPredictor(cfg, transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be2296-2e28-4c0a-84c8-abbc792799a8",
   "metadata": {},
   "source": [
    "## Generate Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e41ef-864d-467a-80c5-1700c1da6c00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in tqdm(names):\n",
    "    # Get index number of file\n",
    "    index = name[:-4]\n",
    "    \n",
    "    # read the image\n",
    "    im = cv2.imread(img_root+name)\n",
    "    \n",
    "    # Get dimensions of image\n",
    "    orig_shape = im.shape[0:2]\n",
    "    \n",
    "    # Resize image\n",
    "    im = cv2.resize(im, res_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Get predictions\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    # Get scores and predicted masks\n",
    "    scores = outputs['instances'].to('cpu').scores.numpy()\n",
    "    pred_masks = outputs['instances'].to('cpu').pred_masks.numpy()\n",
    "    pred_boxes = outputs['instances'].to('cpu').pred_boxes.tensor.numpy()\n",
    "    pred_classes = outputs['instances'].to('cpu').pred_classes.numpy()\n",
    "    \n",
    "    # Initialize counter\n",
    "    count = 1\n",
    "    bbox_list = []\n",
    "    \n",
    "    # Iterate through the detections\n",
    "    for i in range(len(scores)):\n",
    "        # check if detection probability is more than threshold\n",
    "        if scores[i] >= thresh:\n",
    "            # Extract and save the mask\n",
    "            tmp_mask = pred_masks[i].astype('uint8')\n",
    "            tmp_mask = 255 * tmp_mask\n",
    "            tmp_mask = cv2.resize(tmp_mask, orig_shape[::-1],interpolation=cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(pred_root+index+'_'+str(count)+'.bmp', tmp_mask)\n",
    "            count += 1\n",
    "            \n",
    "            # Save the filtered Bounding Boxes\n",
    "            bbox_list.append((pred_boxes[i], pred_classes[i]))\n",
    "    \n",
    "    # Save the detected bounding boxes for each image\n",
    "    with open(f'{pred_root}{index}.pkl', 'wb') as f:\n",
    "        pickle.dump(bbox_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b3b83-97b4-4735-ae89-2d6a45b5d3e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Combine results for cyptoplasm and nucleus detections\n",
    "for name in tqdm(names):\n",
    "    \n",
    "    # Get index name of file\n",
    "    index = name[:-4]\n",
    "\n",
    "    # Load the bounding boxes from the pickle file\n",
    "    with open(pred_root+index+'.pkl', 'rb') as f:\n",
    "        pickle_file = pickle.load(f)\n",
    "    \n",
    "    # Get all bboxes and class predictions\n",
    "    bboxes = [item[0] for item in pickle_file]\n",
    "    bbox_classes = [item[1] for item in pickle_file]\n",
    "    \n",
    "    # Calculate the overlap ratio between all pairs of bounding boxes\n",
    "    overlap_matrix = np.zeros((len(bboxes), len(bboxes)))\n",
    "    for i in range(len(bboxes)):\n",
    "        for j in range(i + 1, len(bboxes)):\n",
    "            bbox_i = bboxes[i]\n",
    "            bbox_j = bboxes[j]\n",
    "            box_i = box(*bbox_i)\n",
    "            box_j = box(*bbox_j)\n",
    "            intersection = box_i.intersection(box_j).area\n",
    "            union = box_i.union(box_j).area\n",
    "            overlap_ratio = intersection / union\n",
    "            overlap_matrix[i, j] = overlap_ratio\n",
    "            overlap_matrix[j, i] = overlap_ratio\n",
    "\n",
    "    # Loop through the overlap matrix and find the overlapping bounding boxes\n",
    "    overlappig_bbs = []\n",
    "    for i in range(len(bboxes)):\n",
    "        for j in range(i + 1, len(bboxes)):\n",
    "            if overlap_matrix[i, j] > 0.2:\n",
    "                overlappig_bbs.append((i,j)) \n",
    "                \n",
    "    # Find the non overlap bb's\n",
    "    all_overlappig_bbs = [item for sublist in overlappig_bbs for item in sublist]\n",
    "    left_bbs = [bb for bb in range(len(bbox_classes)) if bb not in all_overlappig_bbs]\n",
    "    \n",
    "    counter = 1\n",
    "    # Create cytoplasm and nucleus masks\n",
    "    for bb1, bb2 in overlappig_bbs:\n",
    "        # Read the two masks\n",
    "        img1 = io.imread(pred_root+index+'_'+str(bb1+1)+'.bmp', as_gray=True).copy()\n",
    "        img2 = io.imread(pred_root+index+'_'+str(bb2+1)+'.bmp', as_gray=True).copy()\n",
    "        \n",
    "        # get the classes\n",
    "        img1_class = bbox_classes[bb1]\n",
    "        img2_class = bbox_classes[bb2]\n",
    "\n",
    "        # if cytoplasm\n",
    "        if img1_class:\n",
    "            # keep cytoplasm pixels to 20\n",
    "            img_1_index = np.where(img1 == 255)\n",
    "            img1[img_1_index] = 20\n",
    "            \n",
    "            # keep nuclues pixels to 40\n",
    "            img_2_index = np.where(img2 == 255)\n",
    "            img1[img_2_index] = 40\n",
    "            \n",
    "            # save the mask\n",
    "            io.imsave(final_pred_root+index+'_'+str(counter)+'.bmp', img1)\n",
    "        # if nucleus\n",
    "        else:\n",
    "            # keep cytoplasm pixels to 20\n",
    "            img_2_index = np.where(img2 == 255)\n",
    "            img2[img_2_index] = 20\n",
    "            \n",
    "            # keep nuclues pixels to 40\n",
    "            img_1_index = np.where(img1 == 255)\n",
    "            img2[img_1_index] = 40\n",
    "            # save the mask\n",
    "            io.imsave(final_pred_root+index+'_'+str(counter)+'.bmp', img2)\n",
    "        \n",
    "        # Update counter\n",
    "        counter = counter + 1\n",
    "    \n",
    "    # process the single detections of cytoplasm and nuclues\n",
    "    for bb_left in left_bbs:\n",
    "        # read the mask image\n",
    "        img1 = io.imread(pred_root+index+'_'+str(bb_left+1)+'.bmp', as_gray=True).copy()\n",
    "        \n",
    "        # get the predicted class\n",
    "        img1_class = bbox_classes[bb_left]\n",
    "\n",
    "        # if cytoplasm\n",
    "        if img1_class:\n",
    "            # keep cytoplasm pixels to 20\n",
    "            img_1_index = np.where(img1 == 255)\n",
    "            img1[img_1_index] = 20\n",
    "            # save the mask\n",
    "            io.imsave(final_pred_root+index+'_'+str(counter)+'.bmp', img1)\n",
    "        # if nucleus\n",
    "        else:\n",
    "            # keep nuclues pixels to 40\n",
    "            img_1_index = np.where(img1 == 255)\n",
    "            img1[img_1_index] = 40\n",
    "            # save the mask\n",
    "            io.imsave(final_pred_root+index+'_'+str(counter)+'.bmp', img1)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849458a-1ebb-4139-8949-af98a9a4153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create file to submit for SEG-PC Challenge 2021\n",
    "file_path = './submission.txt'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Delete file\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' deleted.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1767f-79ee-48f6-974a-92852bcacab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create submissions.txt\n",
    "! /hpc/group/aipi540-s23/sl808/miniconda3/envs/bme548_fp/bin/python submission.py -s $model_final_results -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319eda8-4644-4b5b-96d8-89ec75ad6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zip the submissions file\n",
    "! zip --verbose $transform_type submission.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1106438-a3e4-4024-942d-7af21f0feb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bme548_fp",
   "language": "python",
   "name": "bme548_fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
