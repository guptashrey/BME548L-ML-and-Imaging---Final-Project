{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f9b0f-a152-483b-84a0-4cf2ec864a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import detectron2\n",
    "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data.transforms import Transform\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.modeling import build_model,build_resnet_backbone,build_backbone\n",
    "from detectron2.structures import ImageList, Instances\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "\n",
    "from transformations import *\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf227b-94e8-4c84-b23f-632cd6c40f6d",
   "metadata": {},
   "source": [
    "## Transformation Functions\n",
    "\n",
    "- Contrast normalization: The myeloma cells have varying levels of contrast as compared to other cells and tissues, so normalizing the contrast can help them be more visible\n",
    "- Morphological operations: Erosion or dilation operations can smooth the edges of the cells can help us detect the cancer cells better\n",
    "- Gradient Filters: Sobel filter can help identify the boundaries of the cells better\n",
    "- Color Channels: Manipulate the different color channels (RGB) of the image by suppressing or enhancing the effect of either red, green, or blue channel\n",
    "- Blur Filter: Try the Gaussian blur filters to smooth the image and reduce the noise\n",
    "- Resolution: Benchmark the accuracy of detecting myeloma cells by reducing the resolution of the image to see if we can get at par accuracy with a smaller dimension image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc74e3d-5f4c-4b1f-98f8-93f243b4fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformation(transformation_type=None):\n",
    "    \"\"\"\n",
    "    Define the type of augmentation to apply to the images.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    \"\"\"\n",
    "    if transformation_type is None:\n",
    "        transformation = NoOpTransform()\n",
    "        \n",
    "    else:\n",
    "        transformation = eval(transformation_type + \"()\")\n",
    "    \n",
    "    return [transformation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35d63b-cd36-41cb-90f1-f8aaa4abd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformation(img_path=\"../TCIA_SegPC_dataset/coco/x/106.bmp\", transformation_type=None):\n",
    "    \"\"\"\n",
    "    Display the transformation on the image\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    ## Convert to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ## Get the image transformer\n",
    "    transform_object = select_transformation(transformation_type)[0]\n",
    "    \n",
    "    ## APply transformation\n",
    "    img_transformed = transform_object.apply_image(img)\n",
    "    \n",
    "    ## Create a figure with two subplots in a single row\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "\n",
    "    ## Display the first image in the first subplot\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title('Original Image')\n",
    "\n",
    "    ## Display the second image in the second subplot\n",
    "    axs[1].imshow(img_transformed)\n",
    "    axs[1].set_title('Transformed Image')\n",
    "\n",
    "    ## Show the figure\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d2e91-fa8d-414a-8cca-47059c2eb3f0",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559fd5f-7e64-4e5d-b18d-f2bd219d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define netowrk architecture\n",
    "@BACKBONE_REGISTRY.register()\n",
    "class Effb5(Backbone):\n",
    "    def __init__(self, cfg, input_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up the illumination layer\n",
    "#         self.illumination = nn.Conv2d(in_channels=3, out_channels=9, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "#         torch.nn.init.normal_(self.illumination.weight, mean=0.0, std=0.05)\n",
    "#         in_channels = 1\n",
    "        \n",
    "        in_channels = 3\n",
    "        encoder_name = 'timm-efficientnet-b5'\n",
    "        encoder_depth = 5\n",
    "        encoder_weights = 'noisy-student'\n",
    "        self.encoder = get_encoder(encoder_name,\n",
    "                in_channels=in_channels,\n",
    "                depth=encoder_depth,\n",
    "                weights=encoder_weights)\n",
    "        self.channels = self.encoder.out_channels\n",
    "        self.conv = nn.ModuleList(\n",
    "            [nn.Conv2d(self.channels[i],256,3,stride = 2, padding = 1) for i in range(len(self.channels))]\n",
    "        )\n",
    "\n",
    "        self.names = [\"p\"+str(i+1) for i in range(6)]\n",
    "        \n",
    "    def forward(self, image):\n",
    "\n",
    "#         illuminated_image = torch.sum(self.illumination(image), dim=1, keepdim=True)\n",
    "#         features = self.encoder(illuminated_image)\n",
    "        features = self.encoder(image)\n",
    "        out = {self.names[i]: self.conv[i](features[i]) for i in range(1, len(features))}\n",
    "\n",
    "        return out\n",
    "    def output_shape(self):\n",
    "        out_shape = {self.names[i]: ShapeSpec(channels =256, stride = 2**(i+1)) for i in range(1, len(self.names))}\n",
    "        return out_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4900d6-3af9-4956-91dd-1bc324f6e2e2",
   "metadata": {},
   "source": [
    "## Model Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f675d6-7296-418b-8dea-845ff96b6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom class for model training\n",
    "    \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "            \n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=select_transformation(TRANSFORM_TYPE))\n",
    "        \n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg):\n",
    "        \n",
    "        mapper = DatasetMapper(cfg, is_train=False, augmentations=select_transformation(TRANSFORM_TYPE))\n",
    "        \n",
    "        return build_detection_test_loader(cfg, mapper=mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89765e3d-8bf2-4ea5-b655-11359f854ec8",
   "metadata": {},
   "source": [
    "## Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f84bf-6295-4cd0-b09d-cf84219abd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"SegPC_train\", {}, \"../TCIA_SegPC_dataset/coco/COCO.json\", \"../TCIA_SegPC_dataset/coco/x/\")\n",
    "register_coco_instances(\"SegPC_val\", {}, \"../TCIA_SegPC_dataset/coco_val/COCO.json\", \"../TCIA_SegPC_dataset/coco_val/x/\")\n",
    "\n",
    "train_meta = MetadataCatalog.get('SegPC_train')\n",
    "val_meta = MetadataCatalog.get('SegPC_val')\n",
    "\n",
    "train_dicts = DatasetCatalog.get(\"SegPC_train\")\n",
    "val_dicts = DatasetCatalog.get(\"SegPC_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf2ca7-21bc-483f-8dc6-b86ae3b7c9de",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731bddc-dd0c-4f17-b89b-65a7ddafc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize model configuration\n",
    "cfg = get_cfg()\n",
    "\n",
    "## Set parameters\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"SegPC_train\",)\n",
    "cfg.DATASETS.TEST = (\"SegPC_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 5\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\")  \n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.02/8\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = 'WarmupCosineLR'\n",
    "\n",
    "cfg.SOLVER.WARMUP_ITERS = 100\n",
    "cfg.SOLVER.MAX_ITER = 3725\n",
    "cfg.SOLVER.STEPS = (1000, 1500)\n",
    "cfg.SOLVER.GAMMA = 0.05\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 250\n",
    "cfg.MODEL.BACKBONE.NAME = \"Effb5\"\n",
    "\n",
    "cfg.CUDNN_BENCHMARK = True\n",
    "cfg.OUTPUT_DIR = \"./output/\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TRANSFORM_TYPE = \"GaussianBlur\" #ContrastNormalization, GaussianBlur, CorrectColor, Dilation, Erosion, SobelFilterX, SobelFilterY, EnhanceRedColor, EnhanceGreenColor, EnhanceBlueColor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8e1ac-4174-44f7-b957-a5ab682a4455",
   "metadata": {},
   "source": [
    "## Testing Trasformation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a856b3-37dc-4fb6-b342-7977e59cb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformation(transformation_type=TRANSFORM_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287c780-3b8c-4aeb-8397-dba45afded71",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f52bf-2bfd-4ddb-914b-80fa8b227d0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Train the model\n",
    "# trainer = CocoTrainer(cfg)\n",
    "# trainer.resume_or_load(resume=False)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e89c6-b0fd-4aec-80e5-3292f2732bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
