{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b513daa2",
   "metadata": {},
   "source": [
    "### 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f9b0f-a152-483b-84a0-4cf2ec864a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "from helpers import *\n",
    "\n",
    "# standard imports\n",
    "import os\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# detectron2 imports\n",
    "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from segmentation_models_pytorch.encoders import get_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14cf227b-94e8-4c84-b23f-632cd6c40f6d",
   "metadata": {},
   "source": [
    "### 2. Implemented Image Transformation Functions\n",
    "\n",
    "- Contrast normalization: The myeloma cells have varying levels of contrast as compared to other cells and tissues, so normalizing the contrast can help them be more visible\n",
    "- Morphological operations: Erosion or dilation operations can smooth the edges of the cells can help us detect the cancer cells better\n",
    "- Gradient Filters: Sobel filter can help identify the boundaries of the cells better\n",
    "- Color Channels: Manipulate the different color channels (RGB) of the image by suppressing or enhancing the effect of either red, green, or blue channel\n",
    "- Blur Filter: Try the Gaussian blur filters to smooth the image and reduce the noise\n",
    "- Resolution: Benchmark the accuracy of detecting myeloma cells by reducing the resolution of the image to see if we can get at par accuracy with a smaller dimension image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15487582",
   "metadata": {},
   "source": [
    "#### a. Selecting the image transformation type to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_TYPE = \"IlluminationSimulation\" #IlluminationSimulation, ContrastNormalization, GaussianBlur, CorrectColor, Dilation, Erosion, SobelFilterX, SobelFilterY, EnhanceRedColor, EnhanceGreenColor, EnhanceBlueColor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b4b2a8a",
   "metadata": {},
   "source": [
    "#### b. Testing the selected image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformation(transformation_type=TRANSFORM_TYPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bca7f52",
   "metadata": {},
   "source": [
    "### 3. Detectron2 MASK RCNN Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4231a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize model configuration\n",
    "cfg = get_cfg()\n",
    "\n",
    "## Set parameters\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"SegPC_train\",)\n",
    "cfg.DATASETS.TEST = (\"SegPC_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\")  \n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.02/8\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = 'WarmupCosineLR'\n",
    "\n",
    "cfg.SOLVER.WARMUP_ITERS = 100\n",
    "cfg.SOLVER.MAX_ITER = 3725\n",
    "cfg.SOLVER.STEPS = (1000, 1500)\n",
    "cfg.SOLVER.GAMMA = 0.05\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 250\n",
    "cfg.MODEL.BACKBONE.NAME = \"Effb5\"\n",
    "\n",
    "cfg.CUDNN_BENCHMARK = True\n",
    "cfg.OUTPUT_DIR = f\"../outputs/{TRANSFORM_TYPE}/\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if TRANSFORM_TYPE == \"IlluminationSimulation\":\n",
    "    # Additional parameters for \"IlluminationSimulation Transform\"\n",
    "    cfg.MODEL.PIXEL_MEAN = [0.5]\n",
    "    cfg.MODEL.PIXEL_STD = [1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "818d2e91-fa8d-414a-8cca-47059c2eb3f0",
   "metadata": {},
   "source": [
    "### 4. Initializing the Backbone Layer Architecture for MASK RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559fd5f-7e64-4e5d-b18d-f2bd219d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORM_TYPE == \"IlluminationSimulation\":\n",
    "    class NonNegativeConv2d(nn.Conv2d):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "            super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            self.weight.data.clamp_min_(0.0)\n",
    "            return nn.functional.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "    # Define netowrk architecture\n",
    "    @BACKBONE_REGISTRY.register()\n",
    "    class Effb5(Backbone):\n",
    "        def __init__(self, cfg, input_shape):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Set up the illumination layer\n",
    "            self.illumination = NonNegativeConv2d(in_channels=12, out_channels=12, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            torch.nn.init.normal_(self.illumination.weight, mean=0.0, std=0.05)\n",
    "            \n",
    "            in_channels = 1\n",
    "            encoder_name = 'timm-efficientnet-b5'\n",
    "            encoder_depth = 5\n",
    "            encoder_weights = 'noisy-student'\n",
    "            self.encoder = get_encoder(encoder_name,\n",
    "                    in_channels=in_channels,\n",
    "                    depth=encoder_depth,\n",
    "                    weights=encoder_weights)\n",
    "            self.channels = self.encoder.out_channels\n",
    "            self.conv = nn.ModuleList(\n",
    "                [nn.Conv2d(self.channels[i],256,3,stride = 2, padding = 1) for i in range(len(self.channels))]\n",
    "            )\n",
    "            self.names = [\"p\"+str(i+1) for i in range(6)]\n",
    "            \n",
    "        def forward(self, image):\n",
    "\n",
    "            illuminated_image = torch.sum(self.illumination(image), dim=1, keepdim=True)\n",
    "            features = self.encoder(illuminated_image)\n",
    "            out = {self.names[i]: self.conv[i](features[i]) for i in range(1, len(features))}\n",
    "            return out\n",
    "\n",
    "        def output_shape(self):\n",
    "            out_shape = {self.names[i]: ShapeSpec(channels =256, stride = 2**(i+1)) for i in range(1, len(self.names))}\n",
    "            return out_shape\n",
    "        \n",
    "else:\n",
    "    # Define netowrk architecture\n",
    "    @BACKBONE_REGISTRY.register()\n",
    "    class Effb5(Backbone):\n",
    "        def __init__(self, cfg, input_shape):\n",
    "            super().__init__()\n",
    "            \n",
    "            in_channels = 3\n",
    "            encoder_name = 'timm-efficientnet-b5'\n",
    "            encoder_depth = 5\n",
    "            encoder_weights = 'noisy-student'\n",
    "            self.encoder = get_encoder(encoder_name,\n",
    "                    in_channels=in_channels,\n",
    "                    depth=encoder_depth,\n",
    "                    weights=encoder_weights)\n",
    "            self.channels = self.encoder.out_channels\n",
    "            self.conv = nn.ModuleList(\n",
    "                [nn.Conv2d(self.channels[i],256,3,stride = 2, padding = 1) for i in range(len(self.channels))]\n",
    "            )\n",
    "            self.names = [\"p\"+str(i+1) for i in range(6)]\n",
    "            \n",
    "        def forward(self, image):\n",
    "\n",
    "            features = self.encoder(image)\n",
    "            out = {self.names[i]: self.conv[i](features[i]) for i in range(1, len(features))}\n",
    "            return out\n",
    "\n",
    "        def output_shape(self):\n",
    "            out_shape = {self.names[i]: ShapeSpec(channels =256, stride = 2**(i+1)) for i in range(1, len(self.names))}\n",
    "            return out_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4900d6-3af9-4956-91dd-1bc324f6e2e2",
   "metadata": {},
   "source": [
    "### 5. Overloading the \"build_train_loader\" & \"build_test_loader\" methods of \"DefaultTrainer\" Class to add image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd036885-7978-4670-8553-5bb1ba97f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom class for model training\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "            \n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=select_transformation(TRANSFORM_TYPE))\n",
    "        \n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        \n",
    "        mapper = DatasetMapper(cfg, is_train=False, augmentations=select_transformation(TRANSFORM_TYPE))\n",
    "        \n",
    "        return build_detection_test_loader(cfg, dataset_name, mapper=mapper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89765e3d-8bf2-4ea5-b655-11359f854ec8",
   "metadata": {},
   "source": [
    "### 6. Initializing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f84bf-6295-4cd0-b09d-cf84219abd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"SegPC_train\", {}, \"../TCIA_SegPC_dataset/coco/COCO.json\", \"../TCIA_SegPC_dataset/coco/x/\")\n",
    "register_coco_instances(\"SegPC_val\", {}, \"../TCIA_SegPC_dataset/coco_val/COCO.json\", \"../TCIA_SegPC_dataset/coco_val/x/\")\n",
    "\n",
    "train_meta = MetadataCatalog.get('SegPC_train')\n",
    "val_meta = MetadataCatalog.get('SegPC_val')\n",
    "\n",
    "train_dicts = DatasetCatalog.get(\"SegPC_train\")\n",
    "val_dicts = DatasetCatalog.get(\"SegPC_val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a287c780-3b8c-4aeb-8397-dba45afded71",
   "metadata": {},
   "source": [
    "### 7. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f52bf-2bfd-4ddb-914b-80fa8b227d0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train the model\n",
    "trainer = CocoTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bme",
   "language": "python",
   "name": "bme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
